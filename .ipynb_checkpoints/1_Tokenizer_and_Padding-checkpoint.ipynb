{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer and padding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In NLP, our input data is text, but neural networks are crazy for numbers and don't deal with texts. The tokenizer, helps us in this scenario. \n",
    "\n",
    "Here is a [link to the documentation](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessingprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#some sentences\n",
    "sentences = [\n",
    "    \"I am learning Tensorflow\",\n",
    "    \"I am learning Keras\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializing the tokenizer\n",
    "tokenizer = Tokenizer(num_words = 100)\n",
    "#tokenizing the sentences\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'i': 1, 'am': 2, 'learning': 3, 'tensorflow': 4, 'keras': 5}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's print it\n",
    "word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, each word is mapped to  a number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with \"Out of Vocabulary\" words\n",
    "\n",
    "We will encode all the words which are not present in our training data, as an out_of_vocabulary word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining a new tokenizer\n",
    "new_tokenizer = Tokenizer(num_words = 100, oov_token = \"<OOV>\")\n",
    "new_tokenizer.fit_on_texts(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words mapped to : {'<OOV>': 1, 'i': 2, 'am': 3, 'learning': 4, 'tensorflow': 5, 'keras': 6}\n",
      "\n",
      "['I am learning Tensorflow', 'I am learning Keras'] mapped to [[2, 3, 4, 5], [2, 3, 4, 6]]\n"
     ]
    }
   ],
   "source": [
    "#printing entire sequences\n",
    "print(\"words mapped to :\", new_tokenizer.word_index, end='\\n\\n')\n",
    "print(sentences, \"mapped to\", new_tokenizer.texts_to_sequences(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sentence = [\"I am learning NLP\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I am learning NLP'] transformed to [[1, 2, 3]] by tokenizer\n",
      "['I am learning NLP'] transformed to [[2, 3, 4, 1]] by new tokenizer\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    texts_to_sequences(texts) \n",
    "    transforms each text in \"texts\" ot a sequence of integers\n",
    "'''\n",
    "sequence = tokenizer.texts_to_sequences(new_sentence)\n",
    "new_sequence = new_tokenizer.texts_to_sequences(new_sentence)\n",
    "\n",
    "print(new_sentence, \"transformed to\", sequence, \"by tokenizer\")\n",
    "print(new_sentence, \"transformed to\", new_sequence , \"by new tokenizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, the tokenizer simply rejects the \"out of vocabulary\" words, if they are not passed the oov_token. But it is mapped to 1, if oov_token is passed(make sure it is unique).Example, above since \"NLP\" is not in sentences, hence it is marked as '<OOV>'or 1 by tokenizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with variable sized sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"I am dancing\", \n",
    "    \"I am singing\", \n",
    "    \"Will you dance with me?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2, 4], [1, 2, 5], [6, 7, 8, 9, 10]]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.fit_on_texts(sentences)\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "padded sequences\n",
      "\n",
      "sequences with default max length\n",
      "[[ 0  0  1  2  4]\n",
      " [ 0  0  1  2  5]\n",
      " [ 6  7  8  9 10]]\n",
      "\n",
      "sequences with max length = 8\n",
      "[[ 0  0  0  0  0  1  2  4]\n",
      " [ 0  0  0  0  0  1  2  5]\n",
      " [ 0  0  0  6  7  8  9 10]]\n",
      "\n",
      "sequences with post padding\n",
      "[[ 1  2  4  0  0  0  0  0]\n",
      " [ 1  2  5  0  0  0  0  0]\n",
      " [ 6  7  8  9 10  0  0  0]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Padded sequences\", end = '\\n\\n')\n",
    "\n",
    "pad_sequence_1 = pad_sequences(sequences)\n",
    "print('sequences with default max length')\n",
    "print(pad_sequence_1, end='\\n\\n')\n",
    "\n",
    "pad_sequence_2 = pad_sequences(sequences, maxlen = 8)\n",
    "print('sequences with max length = 8')\n",
    "print(pad_sequence_2, end='\\n\\n')\n",
    "\n",
    "pad_sequence_3 = pad_sequences(sequences, maxlen= 8, padding = 'post')\n",
    "print('sequences with post padding')\n",
    "print(pad_sequence_3, end='\\n\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
